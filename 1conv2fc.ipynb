{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#导入所需的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "FashionMNIST数据集\n",
    "60000张训练图像和对应Label；\n",
    "10000张测试图像和对应Label；\n",
    "10个类别；\n",
    "每张图像28x28的分辨率；\n",
    "'''\n",
    "\n",
    "\n",
    "#设置随机种子，确保每次训练结果一致\n",
    "torch.manual_seed(22)\n",
    "\n",
    "#训练集和测试集下载\n",
    "train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "#只取前500个测试数据\n",
    "#test_data_subset = Subset(test_data, indices=range(500))\n",
    "#test_loader = torch.utils.data.DataLoader(test_data_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "#加载训练集和测试集\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 获取一批训练数据\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# 选择要查看的图像索引\n",
    "img_index = 1\n",
    "\n",
    "# 将图像从 Tensor 转换为 NumPy 以进行可视化\n",
    "image = images[img_index].numpy()\n",
    "image = np.transpose(image, (1, 2, 0))  # 将图像的维度从 (channels, height, width) 转换为 (height, width, channels)\n",
    "\n",
    "# 显示图像和对应的标签\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(), cmap='gray')  # 使用灰度图显示图像\n",
    "plt.title('Label: ' + class_names[labels[img_index]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:0.6079745145400365 time:17.278925895690918s\n",
      "epoch:2 loss:0.4038346267024676 time:13.541803121566772s\n",
      "epoch:3 loss:0.35816035843690236 time:15.708700180053711s\n",
      "epoch:4 loss:0.33204573920170466 time:16.214303731918335s\n",
      "epoch:5 loss:0.3098472408890724 time:15.309302568435669s\n",
      "epoch:6 loss:0.2961870493590832 time:18.2295503616333s\n",
      "epoch:7 loss:0.2818114511569341 time:13.866973638534546s\n",
      "epoch:8 loss:0.27128775896430013 time:13.87001895904541s\n",
      "epoch:9 loss:0.26202952651778855 time:13.924323081970215s\n",
      "epoch:10 loss:0.25439195819099747 time:13.45766019821167s\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "\n",
    "#GPU加速\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#搭建神经网络\n",
    "class FashionCNN(nn.Module):\n",
    "    #初始化方法，搭建卷积层和全连接层\n",
    "    def __init__(self,hidden=64,output=10):\n",
    "        super(FashionCNN,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels=1,out_channels=4,kernel_size=7,padding=0,stride=3)\n",
    "        self.fc1=nn.Linear(256,hidden)\n",
    "        self.fc2=nn.Linear(hidden,output)\n",
    "    #前向传播\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=x*x\n",
    "        x=x.view(-1,256)\n",
    "        x=self.fc1(x)\n",
    "        x=x*x\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#训练模型函数\n",
    "def train(model,train_loader,criterion,optimizer,epochs=10):\n",
    "    #将模型转为训练模式\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0.0\n",
    "        start=time.time() #记录开始时间\n",
    "        for train,target in train_loader:\n",
    "            train, target = train.to(device), target.to(device)\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            output=model(train) #前向传播\n",
    "            loss=criterion(output,target) #计算损失\n",
    "            loss.backward() #反向传播\n",
    "            optimizer.step() #更新参数\n",
    "            train_loss+=loss.item() #总损失\n",
    "        end=time.time()\n",
    "        train_loss/=len(train_loader)\n",
    "        print(f\"epoch:{epoch+1} loss:{train_loss} time:{end-start}s\")\n",
    "    #模型转为评估模式\n",
    "    model.train()\n",
    "\n",
    "model=FashionCNN()\n",
    "model=model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#optimizer=optim.SGD(model.parameters(),lr=0.001)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "train(model,train_loader,criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "PATH = './Fashion_MNIST_Net.pth'\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# 只保存模型权重\n",
    "PATH = './Fashion_MNIST_WeightNet.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.393406\n",
      "\n",
      "Test Accuracy of 0: 86% (866/1000)\n",
      "Test Accuracy of 1: 96% (966/1000)\n",
      "Test Accuracy of 2: 86% (862/1000)\n",
      "Test Accuracy of 3: 92% (927/1000)\n",
      "Test Accuracy of 4: 76% (763/1000)\n",
      "Test Accuracy of 5: 91% (916/1000)\n",
      "Test Accuracy of 6: 57% (570/1000)\n",
      "Test Accuracy of 7: 90% (904/1000)\n",
      "Test Accuracy of 8: 95% (957/1000)\n",
      "Test Accuracy of 9: 97% (975/1000)\n",
      "\n",
      "Test Accuracy (Overall): 87% (8706/10000)\n"
     ]
    }
   ],
   "source": [
    "#评估模型\n",
    "def evaluate(model,criterion,test_loader):\n",
    "    eval_loss=0.0\n",
    "    class_total=list(0.0 for i in range(10))\n",
    "    class_correct=list(0.0 for i in range(10))\n",
    "    #评估模式\n",
    "    model.eval()\n",
    "    for data,target in test_loader:\n",
    "        output=model(data)\n",
    "        loss=criterion(output,target)\n",
    "        eval_loss+=loss.item()\n",
    "        _,pred=torch.max(output,1)\n",
    "        correct=np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # 计算每个对象类别的测试准确率\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # 计算平均测试损失\n",
    "    test_loss = eval_loss / len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    # 打印每个类别的测试准确率\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    # 打印整体测试准确率\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "\n",
    "# 加载模型\n",
    "PATH = './Fashion_MNIST_Net.pth'\n",
    "model = torch.load(PATH)\n",
    "model=model.cpu()\n",
    "# 调用测试函数\n",
    "evaluate(model,criterion,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
    "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
    "    - you can use + operator to add a plain vector as a bias.\n",
    "    - .conv2d_im2col() method is doing a single convolution operation.\n",
    "    - .square_() just square the encrypted vector inplace.\n",
    "\"\"\"\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "\n",
    "class EncConvNet:\n",
    "    def __init__(self, torch_nn):\n",
    "        self.conv_weight = torch_nn.conv.weight.data.view(\n",
    "            torch_nn.conv.out_channels, torch_nn.conv.kernel_size[0],\n",
    "            torch_nn.conv.kernel_size[1]\n",
    "        ).tolist()\n",
    "        self.conv_bias = torch_nn.conv.bias.data.tolist()\n",
    "        \n",
    "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
    "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
    "        \n",
    "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
    "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
    "\n",
    "        \n",
    "    def forward(self, enc_x, windows_nb):\n",
    "        # conv layer\n",
    "        enc_channels = []\n",
    "        for kernel, bias in zip(self.conv_weight, self.conv_bias):\n",
    "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
    "            enc_channels.append(y)\n",
    "        # pack all channels into a single flattened vector\n",
    "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc1 layer\n",
    "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc2 layer\n",
    "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
    "        return enc_x\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    \n",
    "def enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    n=0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        n += 1\n",
    "        if n > 100:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        # Encoding and encryption\n",
    "        x_enc, windows_nb = ts.im2col_encoding(\n",
    "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
    "            kernel_shape[1], stride\n",
    "        )\n",
    "        # Encrypted evaluation\n",
    "        enc_output = enc_model(x_enc, windows_nb)\n",
    "        # Decryption of result\n",
    "        output = enc_output.decrypt()\n",
    "        output = torch.tensor(output).view(1, -1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        label = target.data[0]\n",
    "        class_correct[label] += correct.item()\n",
    "        class_total[label] += 1\n",
    "        t2 = time.time()\n",
    "        print(\"{} round time:{}s loss:{}\".format(n, t2 - t1, loss.item()))\n",
    "\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss / sum(class_total)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "\n",
    "\n",
    "# Load one element at a time\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "# required for encoding\n",
    "kernel_shape = model.conv.kernel_size\n",
    "stride = model.conv.stride[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 round time:0.8864238262176514s loss:0.0\n",
      "2 round time:0.7071750164031982s loss:0.02776845544576645\n",
      "3 round time:2.284860610961914s loss:0.0\n",
      "4 round time:2.341377019882202s loss:11.426949501037598\n",
      "5 round time:2.2589271068573s loss:0.00048387263086624444\n",
      "6 round time:2.2319247722625732s loss:0.0\n",
      "7 round time:2.1340582370758057s loss:0.0\n",
      "8 round time:2.1913909912109375s loss:0.033312369138002396\n",
      "9 round time:2.366311550140381s loss:0.001369729870930314\n",
      "10 round time:2.3764467239379883s loss:0.004281046334654093\n",
      "11 round time:2.1739935874938965s loss:0.25379544496536255\n",
      "12 round time:2.1400234699249268s loss:0.21495631337165833\n",
      "13 round time:2.1266210079193115s loss:0.0\n",
      "14 round time:2.1321682929992676s loss:0.25202134251594543\n",
      "15 round time:2.176724433898926s loss:0.04344390332698822\n",
      "16 round time:2.1518638134002686s loss:0.02439958229660988\n",
      "17 round time:2.2150940895080566s loss:0.27395427227020264\n",
      "18 round time:2.1657052040100098s loss:4.768360213347478e-06\n",
      "19 round time:2.249910354614258s loss:0.024344902485609055\n",
      "20 round time:2.1671035289764404s loss:0.0\n",
      "21 round time:2.199895143508911s loss:0.0010265801101922989\n",
      "22 round time:2.13936448097229s loss:0.0896368995308876\n",
      "23 round time:2.1435818672180176s loss:0.0\n",
      "24 round time:2.116058588027954s loss:0.21378150582313538\n",
      "25 round time:2.1269655227661133s loss:1.0116910934448242\n",
      "26 round time:2.0332672595977783s loss:0.22499139606952667\n",
      "27 round time:2.1349668502807617s loss:0.0\n",
      "28 round time:2.158888816833496s loss:0.04963161051273346\n",
      "29 round time:2.083251714706421s loss:0.05866137892007828\n",
      "30 round time:2.113546371459961s loss:0.00036423723213374615\n",
      "31 round time:2.1500444412231445s loss:0.13056229054927826\n",
      "32 round time:2.045163154602051s loss:0.00041559641249477863\n",
      "33 round time:2.1718153953552246s loss:0.442537784576416\n",
      "34 round time:2.5006461143493652s loss:1.1920928244535389e-07\n",
      "35 round time:2.152590036392212s loss:0.035230331122875214\n",
      "36 round time:2.104416608810425s loss:0.30815327167510986\n",
      "37 round time:2.0607550144195557s loss:0.1322828233242035\n",
      "38 round time:2.2094833850860596s loss:0.09054240584373474\n",
      "39 round time:2.2579219341278076s loss:2.291496515274048\n",
      "40 round time:2.1352174282073975s loss:1.578568696975708\n",
      "41 round time:2.047337055206299s loss:0.6679643988609314\n",
      "42 round time:2.1037521362304688s loss:0.002531896810978651\n",
      "43 round time:2.282435894012451s loss:0.0\n",
      "44 round time:2.0925424098968506s loss:1.7253620624542236\n",
      "45 round time:2.083151340484619s loss:0.9265102744102478\n",
      "46 round time:2.132316827774048s loss:0.4846341609954834\n",
      "47 round time:2.096372127532959s loss:0.16542507708072662\n",
      "48 round time:2.138373851776123s loss:0.002659714547917247\n",
      "49 round time:2.0389351844787598s loss:0.0\n",
      "50 round time:2.231804847717285s loss:0.06293164193630219\n",
      "51 round time:2.1843862533569336s loss:4.768370445162873e-07\n",
      "52 round time:2.183293342590332s loss:1.966933996300213e-05\n",
      "53 round time:2.552290201187134s loss:1.567279577255249\n",
      "54 round time:2.4900858402252197s loss:1.165501594543457\n",
      "55 round time:3.008294105529785s loss:0.3652781546115875\n",
      "56 round time:2.337611198425293s loss:1.5808771848678589\n",
      "57 round time:2.4940450191497803s loss:5.245071224635467e-05\n",
      "58 round time:2.6246514320373535s loss:0.018289610743522644\n",
      "59 round time:2.5362226963043213s loss:0.0\n",
      "60 round time:2.8583736419677734s loss:0.13797426223754883\n",
      "61 round time:2.5364973545074463s loss:0.1575964242219925\n",
      "62 round time:2.420320749282837s loss:0.016271408647298813\n",
      "63 round time:2.668525457382202s loss:0.008605184964835644\n",
      "64 round time:2.3192176818847656s loss:0.001035749795846641\n",
      "65 round time:2.7917683124542236s loss:0.39359962940216064\n",
      "66 round time:2.5557427406311035s loss:2.50339189733495e-06\n",
      "67 round time:2.580871820449829s loss:0.0\n",
      "68 round time:2.6892776489257812s loss:6.389413465512916e-05\n",
      "69 round time:2.638416051864624s loss:0.0\n",
      "70 round time:2.6092031002044678s loss:1.0435125827789307\n",
      "71 round time:2.8934974670410156s loss:0.0\n",
      "72 round time:2.2599427700042725s loss:0.029436200857162476\n",
      "73 round time:2.125653028488159s loss:0.0\n",
      "74 round time:2.2160990238189697s loss:0.0010643299901857972\n",
      "75 round time:2.570488929748535s loss:0.0\n",
      "76 round time:2.259925603866577s loss:4.332791328430176\n",
      "77 round time:2.630598545074463s loss:0.21831873059272766\n",
      "78 round time:3.066685438156128s loss:4.553687572479248\n",
      "79 round time:2.557032585144043s loss:0.16786378622055054\n",
      "80 round time:2.281268835067749s loss:2.1960809230804443\n",
      "81 round time:2.517707586288452s loss:0.0\n",
      "82 round time:2.68681263923645s loss:3.4570634852570947e-06\n",
      "83 round time:2.445312023162842s loss:0.0\n",
      "84 round time:2.566413164138794s loss:0.04871189221739769\n",
      "85 round time:2.104992151260376s loss:0.29592984914779663\n",
      "86 round time:2.099365711212158s loss:0.00013469743134919554\n",
      "87 round time:2.1627426147460938s loss:0.4022079408168793\n",
      "88 round time:2.1612908840179443s loss:0.04307710379362106\n",
      "89 round time:2.100017786026001s loss:4.46151876449585\n",
      "90 round time:2.482945203781128s loss:0.14296796917915344\n",
      "91 round time:2.6433815956115723s loss:0.0\n",
      "92 round time:2.6163344383239746s loss:0.0061357938684523106\n",
      "93 round time:2.659816265106201s loss:0.0041427514515817165\n",
      "94 round time:2.3108506202697754s loss:0.004497057292610407\n",
      "95 round time:2.508089065551758s loss:0.000481132126878947\n",
      "96 round time:2.275639295578003s loss:0.19193869829177856\n",
      "97 round time:2.117359161376953s loss:0.03909572958946228\n",
      "98 round time:2.420508623123169s loss:0.007186634000390768\n",
      "99 round time:2.1831791400909424s loss:0.0\n",
      "100 round time:2.1505141258239746s loss:1.2397689715726301e-05\n",
      "Test Loss: 0.468819\n",
      "\n",
      "Test Accuracy of 0: 71% (5/7)\n",
      "Test Accuracy of 1: 100% (10/10)\n",
      "Test Accuracy of 2: 78% (11/14)\n",
      "Test Accuracy of 3: 85% (6/7)\n",
      "Test Accuracy of 4: 90% (10/11)\n",
      "Test Accuracy of 5: 81% (9/11)\n",
      "Test Accuracy of 6: 71% (10/14)\n",
      "Test Accuracy of 7: 87% (7/8)\n",
      "Test Accuracy of 8: 100% (13/13)\n",
      "Test Accuracy of 9: 100% (5/5)\n",
      "\n",
      "Test Accuracy (Overall): 86% (86/100)\n",
      "运行了229.21249628067017s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Choosing the parameters isn't easy, so we list some intuition here for why we have chosen these parameters exactly:\n",
    "\n",
    "1. For a given security level (e.g. 128-bits security) and a polynomial modulus degree (e.g. 8192) there is an upper bound for the bit count of the coefficient modulus (`sum(coeff_mod_bit_sizes)`). If the upper bound is surpassed, there is a need to use a higher polynomial modulus degree (e.g. 16384) in order to make sure we still have the required security level.\n",
    "2. The multiplicative depth is controlled by the number of primes constituting our coefficient modulus.\n",
    "3. All elements of `coeff_mod_bit_sizes[1: -1]` should be equal in TenSEAL, since it takes care of rescaling ciphertexts. And we also want to use the same number of bits (e.g. 2 ^ 26) for the scale during encryption.\n",
    "4. The scale is what controls the precision of the fractional part, since it's the value that plaintexts are multiplied with before being encoded into a polynomial of integer coefficients.\n",
    "\n",
    "Starting with a scale of more than 20 bits, we need to choose the number of bits of all the middle primes equal to that, so we are already over 120 bits. With this lower bound of coefficient modulus and a security level of 128-bits, we will need a polynomial modulus degree of at least 8192. The upper bound for choosing a higher degree is at 218. Trying different values for the precision and adjusting the coefficient modulus, while studying the loss and accuracy, we end up with 26-bits of scale and primes. We also have 5 bits (31 - 26) for the integer part in the last coefficient modulus, which should be enough for our use case, since output values aren't that big.\n",
    "'''\n",
    "\n",
    "## Encryption Parameters\n",
    "\n",
    "# controls precision of the fractional part\n",
    "bits_scale = 26\n",
    "\n",
    "# Create TenSEAL context\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes=[31,  bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
    ")\n",
    "\n",
    "# set the scale\n",
    "context.global_scale = pow(2, bits_scale)\n",
    "\n",
    "# galois keys are required to do ciphertext rotations\n",
    "context.generate_galois_keys()\n",
    "#This will now run encrypted evaluation over the whole test-set. It's gonna take time, but with this, you can feel proud of having done encrypted inference on a test-set of 10000 elements, congratulations!\n",
    "start=time.time()\n",
    "enc_model = EncConvNet(model)\n",
    "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)\n",
    "end=time.time()\n",
    "print(f\"运行了{end-start}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
