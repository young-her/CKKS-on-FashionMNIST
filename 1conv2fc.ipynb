{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#导入所需的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "import torch.optim as optim\n",
    "import tenseal as ts\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "FashionMNIST数据集\n",
    "60000张训练图像和对应Label；\n",
    "10000张测试图像和对应Label；\n",
    "10个类别；\n",
    "每张图像28x28的分辨率；\n",
    "'''\n",
    "\n",
    "\n",
    "#设置随机种子，确保每次训练结果一致\n",
    "torch.manual_seed(22)\n",
    "\n",
    "#训练集和测试集下载\n",
    "train_data = datasets.FashionMNIST(root='data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_data = datasets.FashionMNIST(root='data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "#只取前500个测试数据\n",
    "#test_data_subset = Subset(test_data, indices=range(500))\n",
    "#test_loader = torch.utils.data.DataLoader(test_data_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "#加载训练集和测试集\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 获取一批训练数据\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# 定义类别名称\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "# 选择要查看的图像索引\n",
    "img_index = 1\n",
    "\n",
    "# 将图像从 Tensor 转换为 NumPy 以进行可视化\n",
    "image = images[img_index].numpy()\n",
    "image = np.transpose(image, (1, 2, 0))  # 将图像的维度从 (channels, height, width) 转换为 (height, width, channels)\n",
    "\n",
    "# 显示图像和对应的标签\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(), cmap='gray')  # 使用灰度图显示图像\n",
    "plt.title('Label: ' + class_names[labels[img_index]])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:1 loss:0.6689242488384247 time:11.597928047180176s\n",
      "epoch:2 loss:0.43223303662538526 time:11.802261352539062s\n",
      "epoch:3 loss:0.3798349002202352 time:12.339439153671265s\n",
      "epoch:4 loss:0.34741000193158783 time:12.29508090019226s\n",
      "epoch:5 loss:0.32581202468474707 time:12.238223791122437s\n",
      "epoch:6 loss:0.30956065028508506 time:12.319942235946655s\n",
      "epoch:7 loss:0.29570278622905416 time:12.255208492279053s\n",
      "epoch:8 loss:0.2841845334420601 time:12.262225151062012s\n",
      "epoch:9 loss:0.27322715047895907 time:12.296542882919312s\n",
      "epoch:10 loss:0.26328339286943275 time:12.248384475708008s\n",
      "epoch:11 loss:0.2552214925934871 time:12.267890691757202s\n",
      "epoch:12 loss:0.2471171918739875 time:12.271477699279785s\n",
      "epoch:13 loss:0.24221290095150472 time:12.280086994171143s\n",
      "epoch:14 loss:0.2350539436171452 time:12.244844675064087s\n",
      "epoch:15 loss:0.22905471279968817 time:12.341521739959717s\n"
     ]
    }
   ],
   "source": [
    "#训练模型\n",
    "\n",
    "#GPU加速\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "#搭建神经网络\n",
    "class FashionCNN(nn.Module):\n",
    "    #初始化方法，搭建卷积层和全连接层\n",
    "    def __init__(self,hidden=64,output=10):\n",
    "        super(FashionCNN,self).__init__()\n",
    "        self.conv=nn.Conv2d(in_channels=1,out_channels=4,kernel_size=7,padding=0,stride=3)\n",
    "        self.fc1=nn.Linear(256,hidden)\n",
    "        self.fc2=nn.Linear(hidden,output)\n",
    "    #前向传播\n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        x=x*x\n",
    "        x=x.view(-1,256)\n",
    "        x=self.fc1(x)\n",
    "        x=x*x\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#训练模型函数\n",
    "def train(model,train_loader,criterion,optimizer,epochs=15):\n",
    "    #将模型转为训练模式\n",
    "    model.train()\n",
    "    total_loss=0.0\n",
    "    for epoch in range(epochs):\n",
    "        train_loss=0.0\n",
    "        start=time.time() #记录开始时间\n",
    "        for train,target in train_loader:\n",
    "            train, target = train.to(device), target.to(device)\n",
    "            optimizer.zero_grad() #梯度清零\n",
    "            output=model(train) #前向传播\n",
    "            loss=criterion(output,target) #计算损失\n",
    "            loss.backward() #反向传播\n",
    "            optimizer.step() #更新参数\n",
    "            train_loss+=loss.item() #总损失\n",
    "        end=time.time()\n",
    "        train_loss/=len(train_loader)\n",
    "        print(f\"epoch:{epoch+1} loss:{train_loss} time:{end-start}s\")\n",
    "    #模型转为评估模式\n",
    "    model.train()\n",
    "\n",
    "model=FashionCNN()\n",
    "model=model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "#optimizer=optim.SGD(model.parameters(),lr=0.001)\n",
    "optimizer=optim.Adam(model.parameters(),lr=0.001)\n",
    "train(model,train_loader,criterion,optimizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 保存整个模型\n",
    "PATH = './Fashion_MNIST_Net.pth'\n",
    "torch.save(model, PATH)\n",
    "\n",
    "# 只保存模型权重\n",
    "PATH = './Fashion_MNIST_WeightNet.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.363907\n",
      "\n",
      "Test Accuracy of 0: 81% (811/1000)\n",
      "Test Accuracy of 1: 96% (966/1000)\n",
      "Test Accuracy of 2: 77% (777/1000)\n",
      "Test Accuracy of 3: 87% (875/1000)\n",
      "Test Accuracy of 4: 84% (849/1000)\n",
      "Test Accuracy of 5: 96% (966/1000)\n",
      "Test Accuracy of 6: 69% (696/1000)\n",
      "Test Accuracy of 7: 95% (955/1000)\n",
      "Test Accuracy of 8: 96% (968/1000)\n",
      "Test Accuracy of 9: 96% (960/1000)\n",
      "\n",
      "Test Accuracy (Overall): 88% (8823/10000)\n"
     ]
    }
   ],
   "source": [
    "#评估模型\n",
    "def evaluate(model,criterion,test_loader):\n",
    "    eval_loss=0.0\n",
    "    class_total=list(0.0 for i in range(10))\n",
    "    class_correct=list(0.0 for i in range(10))\n",
    "    #评估模式\n",
    "    model.eval()\n",
    "    for data,target in test_loader:\n",
    "        output=model(data)\n",
    "        loss=criterion(output,target)\n",
    "        eval_loss+=loss.item()\n",
    "        _,pred=torch.max(output,1)\n",
    "        correct=np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # 计算每个对象类别的测试准确率\n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    # 计算平均测试损失\n",
    "    test_loss = eval_loss / len(test_loader)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    # 打印每个类别的测试准确率\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    # 打印整体测试准确率\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% '\n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "\n",
    "# 加载模型\n",
    "PATH = './Fashion_MNIST_Net.pth'\n",
    "model = torch.load(PATH)\n",
    "model=model.cpu()\n",
    "# 调用测试函数\n",
    "evaluate(model,criterion,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It's a PyTorch-like model using operations implemented in TenSEAL.\n",
    "    - .mm() method is doing the vector-matrix multiplication explained above.\n",
    "    - you can use + operator to add a plain vector as a bias.\n",
    "    - .conv2d_im2col() method is doing a single convolution operation.\n",
    "    - .square_() just square the encrypted vector inplace.\n",
    "\"\"\"\n",
    "\n",
    "import tenseal as ts\n",
    "\n",
    "\n",
    "class EncConvNet:\n",
    "    def __init__(self, torch_nn):\n",
    "        self.conv_weight = torch_nn.conv.weight.data.view(\n",
    "            torch_nn.conv.out_channels, torch_nn.conv.kernel_size[0],\n",
    "            torch_nn.conv.kernel_size[1]\n",
    "        ).tolist()\n",
    "        self.conv_bias = torch_nn.conv.bias.data.tolist()\n",
    "        \n",
    "        self.fc1_weight = torch_nn.fc1.weight.T.data.tolist()\n",
    "        self.fc1_bias = torch_nn.fc1.bias.data.tolist()\n",
    "        \n",
    "        self.fc2_weight = torch_nn.fc2.weight.T.data.tolist()\n",
    "        self.fc2_bias = torch_nn.fc2.bias.data.tolist()\n",
    "\n",
    "        \n",
    "    def forward(self, enc_x, windows_nb):\n",
    "        # conv layer\n",
    "        enc_channels = []\n",
    "        for kernel, bias in zip(self.conv_weight, self.conv_bias):\n",
    "            y = enc_x.conv2d_im2col(kernel, windows_nb) + bias\n",
    "            enc_channels.append(y)\n",
    "        # pack all channels into a single flattened vector\n",
    "        enc_x = ts.CKKSVector.pack_vectors(enc_channels)\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc1 layer\n",
    "        enc_x = enc_x.mm(self.fc1_weight) + self.fc1_bias\n",
    "        # square activation\n",
    "        enc_x.square_()\n",
    "        # fc2 layer\n",
    "        enc_x = enc_x.mm(self.fc2_weight) + self.fc2_bias\n",
    "        return enc_x\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    \n",
    "def enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride):\n",
    "    # initialize lists to monitor test loss and accuracy\n",
    "    test_loss = 0.0\n",
    "    class_correct = list(0. for i in range(10))\n",
    "    class_total = list(0. for i in range(10))\n",
    "    n=0\n",
    "\n",
    "    for data, target in test_loader:\n",
    "        n += 1\n",
    "        if n > 50:\n",
    "            break\n",
    "        t1 = time.time()\n",
    "        # Encoding and encryption\n",
    "        x_enc, windows_nb = ts.im2col_encoding(\n",
    "            context, data.view(28, 28).tolist(), kernel_shape[0],\n",
    "            kernel_shape[1], stride\n",
    "        )\n",
    "        # Encrypted evaluation\n",
    "        enc_output = enc_model(x_enc, windows_nb)\n",
    "        # Decryption of result\n",
    "        output = enc_output.decrypt()\n",
    "        output = torch.tensor(output).view(1, -1)\n",
    "\n",
    "        # compute loss\n",
    "        loss = criterion(output, target)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # convert output probabilities to predicted class\n",
    "        _, pred = torch.max(output, 1)\n",
    "        # compare predictions to true label\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n",
    "        # calculate test accuracy for each object class\n",
    "        label = target.data[0]\n",
    "        class_correct[label] += correct.item()\n",
    "        class_total[label] += 1\n",
    "        t2 = time.time()\n",
    "        print(\"{} round time:{}s loss:{}\".format(n, t2 - t1, loss.item()))\n",
    "\n",
    "\n",
    "    # calculate and print avg test loss\n",
    "    test_loss = test_loss / sum(class_total)\n",
    "    print(f'Test Loss: {test_loss:.6f}\\n')\n",
    "\n",
    "    for label in range(10):\n",
    "        print(\n",
    "            f'Test Accuracy of {label}: {int(100 * class_correct[label] / class_total[label])}% '\n",
    "            f'({int(np.sum(class_correct[label]))}/{int(np.sum(class_total[label]))})'\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f'\\nTest Accuracy (Overall): {int(100 * np.sum(class_correct) / np.sum(class_total))}% ' \n",
    "        f'({int(np.sum(class_correct))}/{int(np.sum(class_total))})'\n",
    "    )\n",
    "\n",
    "\n",
    "# Load one element at a time\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "# required for encoding\n",
    "kernel_shape = model.conv.kernel_size\n",
    "stride = model.conv.stride[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 round time:2.9820785522460938s loss:1.5258672647178173e-05\n",
      "2 round time:1.7309787273406982s loss:0.048512835055589676\n",
      "3 round time:1.97397780418396s loss:0.15996915102005005\n",
      "4 round time:1.710634708404541s loss:0.0\n",
      "5 round time:1.8900644779205322s loss:4.768370445162873e-07\n",
      "6 round time:1.5064258575439453s loss:1.1920928244535389e-07\n",
      "7 round time:1.5699453353881836s loss:0.0\n",
      "8 round time:1.7279143333435059s loss:1.3245081901550293\n",
      "9 round time:1.6866967678070068s loss:7.748303323751315e-05\n",
      "10 round time:1.5928080081939697s loss:0.0\n",
      "11 round time:1.5796494483947754s loss:0.021376801654696465\n",
      "12 round time:3.025085210800171s loss:0.035290058702230453\n",
      "13 round time:1.6985280513763428s loss:0.0\n",
      "14 round time:1.6984548568725586s loss:0.0030074152164161205\n",
      "15 round time:1.8034744262695312s loss:0.6376487612724304\n",
      "16 round time:1.7850100994110107s loss:0.11526205390691757\n",
      "17 round time:1.8102176189422607s loss:0.0\n",
      "18 round time:1.7917511463165283s loss:0.022516077384352684\n",
      "19 round time:1.9016821384429932s loss:0.00017987063620239496\n",
      "20 round time:1.8011462688446045s loss:0.00011228884250158444\n",
      "21 round time:1.6950397491455078s loss:0.13834460079669952\n",
      "22 round time:1.897047758102417s loss:0.00031716562807559967\n",
      "23 round time:1.8011462688446045s loss:3.5681209564208984\n",
      "24 round time:1.7940125465393066s loss:0.029125245288014412\n",
      "25 round time:1.8123548030853271s loss:5.27669095993042\n",
      "26 round time:1.8387961387634277s loss:0.0009657248156145215\n",
      "27 round time:1.6945722103118896s loss:1.270094394683838\n",
      "28 round time:1.688863754272461s loss:0.0\n",
      "29 round time:1.7927272319793701s loss:0.0\n",
      "30 round time:1.720792293548584s loss:0.05267876386642456\n",
      "31 round time:1.7805981636047363s loss:2.3841855067985307e-07\n",
      "32 round time:1.7149107456207275s loss:4.342154502868652\n",
      "33 round time:1.7983427047729492s loss:0.0\n",
      "34 round time:1.4946529865264893s loss:0.10716073960065842\n",
      "35 round time:1.6102068424224854s loss:0.02690812386572361\n",
      "36 round time:1.5904715061187744s loss:0.003504684194922447\n",
      "37 round time:1.6899104118347168s loss:0.010772055946290493\n",
      "38 round time:1.8109767436981201s loss:1.255336046218872\n",
      "39 round time:1.7914655208587646s loss:0.0\n",
      "40 round time:1.8865323066711426s loss:0.0\n",
      "41 round time:1.8175525665283203s loss:0.0018199799815192819\n",
      "42 round time:1.7760465145111084s loss:5.686121585313231e-05\n",
      "43 round time:1.8173861503601074s loss:1.0661399364471436\n",
      "44 round time:1.6976087093353271s loss:0.008973981253802776\n",
      "45 round time:1.7953193187713623s loss:0.21837277710437775\n",
      "46 round time:1.694756031036377s loss:0.7882254719734192\n",
      "47 round time:1.8009083271026611s loss:0.0\n",
      "48 round time:1.7078723907470703s loss:0.0003831844369415194\n",
      "49 round time:1.800994634628296s loss:2.7418097943154862e-06\n",
      "50 round time:1.5921945571899414s loss:0.099104143679142\n",
      "Test Loss: 0.412675\n",
      "\n",
      "Test Accuracy of 0: 66% (2/3)\n",
      "Test Accuracy of 1: 85% (6/7)\n",
      "Test Accuracy of 2: 66% (2/3)\n",
      "Test Accuracy of 3: 100% (3/3)\n",
      "Test Accuracy of 4: 75% (3/4)\n",
      "Test Accuracy of 5: 100% (3/3)\n",
      "Test Accuracy of 6: 66% (4/6)\n",
      "Test Accuracy of 7: 100% (8/8)\n",
      "Test Accuracy of 8: 85% (6/7)\n",
      "Test Accuracy of 9: 83% (5/6)\n",
      "\n",
      "Test Accuracy (Overall): 84% (42/50)\n",
      "运行了89.79982209205627s\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Choosing the parameters isn't easy, so we list some intuition here for why we have chosen these parameters exactly:\n",
    "\n",
    "1. For a given security level (e.g. 128-bits security) and a polynomial modulus degree (e.g. 8192) there is an upper bound for the bit count of the coefficient modulus (`sum(coeff_mod_bit_sizes)`). If the upper bound is surpassed, there is a need to use a higher polynomial modulus degree (e.g. 16384) in order to make sure we still have the required security level.\n",
    "2. The multiplicative depth is controlled by the number of primes constituting our coefficient modulus.\n",
    "3. All elements of `coeff_mod_bit_sizes[1: -1]` should be equal in TenSEAL, since it takes care of rescaling ciphertexts. And we also want to use the same number of bits (e.g. 2 ^ 26) for the scale during encryption.\n",
    "4. The scale is what controls the precision of the fractional part, since it's the value that plaintexts are multiplied with before being encoded into a polynomial of integer coefficients.\n",
    "\n",
    "Starting with a scale of more than 20 bits, we need to choose the number of bits of all the middle primes equal to that, so we are already over 120 bits. With this lower bound of coefficient modulus and a security level of 128-bits, we will need a polynomial modulus degree of at least 8192. The upper bound for choosing a higher degree is at 218. Trying different values for the precision and adjusting the coefficient modulus, while studying the loss and accuracy, we end up with 26-bits of scale and primes. We also have 5 bits (31 - 26) for the integer part in the last coefficient modulus, which should be enough for our use case, since output values aren't that big.\n",
    "'''\n",
    "\n",
    "## Encryption Parameters\n",
    "\n",
    "# controls precision of the fractional part\n",
    "bits_scale = 26\n",
    "\n",
    "# Create TenSEAL context\n",
    "context = ts.context(\n",
    "    ts.SCHEME_TYPE.CKKS,\n",
    "    poly_modulus_degree=8192,\n",
    "    coeff_mod_bit_sizes=[31,  bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, bits_scale, 31]\n",
    ")\n",
    "\n",
    "# set the scale\n",
    "context.global_scale = pow(2, bits_scale)\n",
    "\n",
    "# galois keys are required to do ciphertext rotations\n",
    "context.generate_galois_keys()\n",
    "#This will now run encrypted evaluation over the whole test-set. It's gonna take time, but with this, you can feel proud of having done encrypted inference on a test-set of 10000 elements, congratulations!\n",
    "start=time.time()\n",
    "enc_model = EncConvNet(model)\n",
    "enc_test(context, enc_model, test_loader, criterion, kernel_shape, stride)\n",
    "end=time.time()\n",
    "print(f\"运行了{end-start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
